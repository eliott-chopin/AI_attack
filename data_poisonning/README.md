# ğŸ›¡ï¸ AI Red Teaming â€“ Attaques de Data Poisoning & Backdoor  
Projet dÃ©montrant trois types dâ€™attaques sur un modÃ¨le Machine Learning.  
Usage strictement pÃ©dagogique (AI Red Teaming, recherche, robustesse).  
Toutes les donnÃ©es utilisÃ©es sont synthÃ©tiques.

---

# ğŸ“Œ Contenu du dÃ©pÃ´t

Ce repository contient trois scripts Python distincts :

1. label_flipping_and_feature_injection.py  
   Contient deux attaques de data poisoning :  
   - Label Flipping : inversion malveillante dâ€™une partie des Ã©tiquettes  
   - Feature Injection : ajout de fausses donnÃ©es artificielles pour perturber la frontiÃ¨re de dÃ©cision  

2. clean_label_poisoning.py  
   Attaque furtive de clean-label oÃ¹ les labels NE sont PAS modifiÃ©s.  
   On dÃ©place des points de maniÃ¨re stratÃ©gique pour tromper le modÃ¨le.

3. backdoor_trigger.py  
   ImplÃ©mentation dâ€™une behavioral backdoor.  
   Le modÃ¨le fonctionne normalement, mais lorsquâ€™un trigger est prÃ©sent dans lâ€™entrÃ©e, il exÃ©cute :  
   print("hello world")

---

# ğŸš€ 1. Label Flipping & Feature Injection

Ces deux attaques modifient directement le dataset avant lâ€™entraÃ®nement.

## Label Flipping
Fonctionnement :  
- SÃ©lection dâ€™une fraction du dataset  
- Inversion des labels (0 devient 1, 1 devient 0)  
- Le modÃ¨le apprend alors une frontiÃ¨re biaisÃ©e

## Feature Injection
Fonctionnement :  
- CrÃ©ation de nouvelles donnÃ©es artificielles "malveillantes"  
- Ajout Ã  la classe choisie  
- Perturbation de la frontiÃ¨re de dÃ©cision

---

# ğŸš€ 2. Clean-Label Poisoning

Attaque trÃ¨s furtive :  
- Ne modifie pas les labels  
- DÃ©place subtilement certains points pour biaiser la frontiÃ¨re  
- Difficile Ã  dÃ©tecter par inspection humaine ou statistique

---

# ğŸš€ 3. Backdoor Attack (Trigger â†’ Comportement cachÃ©)

Câ€™est une behavioral backdoor :  
- Le modÃ¨le fonctionne normalement  
- MAIS si un trigger apparaÃ®t, le modÃ¨le exÃ©cute une action cachÃ©e  
Ici : print("hello world")

Le trigger utilisÃ© se base sur une condition simple dans les features (ex : une feature dÃ©passant un seuil).

---

# ğŸ¯ Objectifs du projet

Ce projet permet :

- dâ€™Ã©tudier plusieurs attaques de poisoning  
- dâ€™observer leurs impacts sur les performances dâ€™un modÃ¨le  
- de comprendre le fonctionnement dâ€™une backdoor comportementale  
- de comparer attaques bruyantes, furtives et ciblÃ©es  
- de pratiquer des techniques dâ€™AI Red Teaming

---

# ğŸ§ª Utilisation

Chaque script peut Ãªtre exÃ©cutÃ© indÃ©pendamment :

python label_flipping_and_feature_injection.py  
python clean_label_poisoning.py  
python backdoor_trigger.py  

Chaque fichier :

- gÃ©nÃ¨re un dataset propre  
- applique une attaque  
- entraÃ®ne un modÃ¨le clean / empoisonnÃ©  
- compare leurs performances  
- affiche Ã©ventuellement des visualisations

---

# ğŸ›¡ï¸ Disclaimer

Ce projet est strictement rÃ©servÃ© Ã  lâ€™enseignement, la recherche et lâ€™expÃ©rimentation en environnement contrÃ´lÃ©.  
Nâ€™utilisez jamais ces techniques hors cadre lÃ©gal et Ã©thique.

---

# ğŸ¤ Contributions

Suggestions bienvenues pour :  
- ajouter dâ€™autres attaques (backdoor clean-label, TrojanNN, BadNets)  
- intÃ©grer des dÃ©fenses (Neural Cleanse, Spectral Signatures)  
- amÃ©liorer le contenu pÃ©dagogique
